# An谩lisis de autor铆a de Textos en Prosa Latina
Este c贸digo es una adptaci贸n a un front-end streamlit del proyecto base.

## Referencias y cr茅ditos

Este proyecto se inspira en el trabajo de Benjamin Nagy sobre estilometr铆a aplicada a textos latinos. Agradecemos especialmente su art铆culo:

> Nagy, Benjamin. *Some stylometric remarks on Ovids Heroides and the Epistula Sapphus*. Digital Scholarship in the Humanities, 2023. [https://doi.org/10.1093/llc/fqac098](https://doi.org/10.1093/llc/fqac098)

El repositorio asociado al art铆culo original est谩 licenciado bajo **CC-BY 4.0**, lo que permite su reutilizaci贸n con atribuci贸n adecuada. Parte del enfoque metodol贸gico y elementos del c贸digo, en particular el sistema de vectorizaci贸n y el metodo de clasificaci贸n, fueron adaptados de dicho trabajo con respeto a esta licencia.

#  Identificador de Autor de Textos

Este sistema aplica t茅cnicas de **Procesamiento de Lenguaje Natural (NLP)** y **Aprendizaje Autom谩tico** para aprender el estilo de escritura de distintos autores. Despu茅s de entrenarse con textos conocidos, puede predecir qui茅n escribi贸 un texto nuevo no etiquetado.

## Construcci贸n del modelo

### 1. **Vectorizaci贸n de texto: TF-IDF con n-grams**

Los textos se convierten en vectores num茅ricos utilizando el modelo de **TF-IDF** (*Term FrequencyInverse Document Frequency*), que pondera la frecuencia de aparici贸n de fragmentos (o *tokens*) dentro de cada documento, considerando su rareza en el conjunto global.

- Se utiliza un `TfidfVectorizer` de `scikit-learn` con `analyzer='char'`, lo que significa que los textos se analizan como secuencias de **caracteres**, no de palabras.
- Se aplican **n-grams de caracteres**, que son cadenas consecutivas de `n` letras. Por ejemplo, el 3-gram de la palabra `"texto"` generar铆a: `"tex"`, `"ext"`, `"xto"`.
- El rango de n-gramas (`ngram_min`, `ngram_max`) es configurable desde la interfaz. Por defecto, se usa de 2 a 4, es decir, 2-gramas, 3-gramas y 4-gramas.

Esta t茅cnica es muy eficaz en la detecci贸n de estilo de autor, ya que capta patrones como ortograf铆a, morfolog铆a, o uso t铆pico de terminaciones o prefijos.

### 2. Reducci贸n de dimensi贸n: TruncatedSVD
Como el espacio generado por los n-grams puede tener decenas de miles de dimensiones, se aplica Truncated Singular Value Decomposition (TruncatedSVD), tambi茅n conocido como LSA (Latent Semantic Analysis).

Esto reduce la complejidad del modelo sin perder las caracter铆sticas m谩s relevantes del estilo de escritura.

### 3. Clasificaci贸n: Nearest Centroid
El clasificador entrenado es un modelo simple y eficiente: Nearest Centroid, que calcula el centroide (punto medio) de cada clase y predice la clase del nuevo texto seg煤n el centroide m谩s cercano.

# Predicci贸n
Una vez entrenado, el modelo transforma los textos de prueba con el mismo vectorizer y svd, y luego predice el autor utilizando el clasificador y se calcula una tabla de probabilidades para cada clase/autores.

# 驴Qu茅 es el clasificador Nearest Centroid?

**Nearest Centroid** es un algoritmo de clasificaci贸n supervisado, simple y eficiente, especialmente 煤til cuando las clases tienen una distribuci贸n bien separada en el espacio vectorial. Este modelo se basa en calcular el centro (o promedio) de los vectores de cada clase, y luego asigna nuevas muestras a la clase cuyo centroide est茅 m谩s cercano.

###  驴C贸mo funciona?

#### Entrenamiento:
Para cada clase (por ejemplo, un autor), el algoritmo calcula el **centroide**, que es el promedio de todos los vectores que pertenecen a esa clase: 渭_c = (1 / N_c) * 危 x_i.

donde:
- `渭_c` es el centroide de la clase `c`.
- `N_c` es el n煤mero de textos de esa clase.
- `x_i` son los vectores TF-IDF reducidos mediante SVD correspondientes a los textos.

#### Predicci贸n:
1. Un nuevo texto se convierte en un vector (mediante TF-IDF y reducci贸n de dimensi贸n con SVD).
2. Se calcula su distancia a cada centroide.
3. Se asigna la clase del centroide m谩s cercano.
